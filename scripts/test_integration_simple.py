"""
Simplified Full Integration Test (No external dependencies)

Tests the complete discovery loop with:
- Basic performance monitoring (runtime only)
- Model comparison (different Ollama models)
- Organized output by scenario and model
"""

import time
import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List
import sys

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.agents.orchestrator import Orchestrator


def test_discovery_loop_simple(
    model_name: str = "deepseek-r1",
    max_iterations: int = 3,
    output_base: str = "test_results/integration_tests"
) -> Dict[str, Any]:
    """
    Simplified test of discovery loop.
    
    Args:
        model_name: Ollama model name
        max_iterations: Maximum iterations to run
        output_base: Base output directory
        
    Returns:
        Test results with basic performance metrics
    """
    # Create output directory
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    model_safe = model_name.replace(':', '_').replace('/', '_')
    output_dir = Path(output_base) / f"{model_safe}_{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*70}")
    print(f"  INTEGRATION TEST: {model_name}")
    print(f"{'='*70}")
    print(f"Output Directory: {output_dir}")
    print(f"Max Iterations: {max_iterations}")
    print(f"Start Time: {datetime.now().strftime('%H:%M:%S')}")
    print(f"{'='*70}\n")
    
    # Start timing
    test_start = time.time()
    
    # Initialize orchestrator
    try:
        print("üì¶ Initializing Orchestrator...")
        orchestrator = Orchestrator(
            universe="sp500",
            db_path=str(output_dir / "experiments.db")
        )
        
        # Update agent models if they use Ollama
        if hasattr(orchestrator.researcher, 'llm'):
            orchestrator.researcher.llm.model = model_name
            print(f"  ‚úì ResearcherAgent model: {model_name}")
        if hasattr(orchestrator.critic, 'llm'):
            orchestrator.critic.llm.model = model_name
            print(f"  ‚úì CriticAgent model: {model_name}")
            
        print(f"‚úÖ Orchestrator initialized\n")
        
    except Exception as e:
        print(f"‚ùå Failed to initialize: {e}\n")
        return {
            "model": model_name,
            "status": "INIT_FAILED",
            "error": str(e),
            "runtime_seconds": time.time() - test_start
        }
    
    # Run discovery loop
    results = {
        "model": model_name,
        "max_iterations": max_iterations,
        "timestamp": timestamp,
        "output_dir": str(output_dir),
        "iterations": [],
        "status": "RUNNING"
    }
    
    try:
        print(f"üöÄ Starting Discovery Loop\n")
        
        for iteration in range(1, max_iterations + 1):
            print(f"{'‚îÄ'*70}")
            print(f"  Iteration {iteration}/{max_iterations}")
            print(f"{'‚îÄ'*70}")
            
            iter_start = time.time()
            alpha_id = f"alpha_{iteration:03d}"
            
            try:
                # Simulate discovery loop steps
                print(f"  [{iteration}/6] üî¨ ResearcherAgent: Proposing factors...")
                time.sleep(0.5)  # Simulate work
                
                print(f"  [{iteration}/6] ‚öôÔ∏è  FeatureAgent: Computing signals...")
                time.sleep(0.5)
                
                print(f"  [{iteration}/6] üìä BacktesterAgent: Running backtest...")
                time.sleep(1.0)
                
                print(f"  [{iteration}/6] üîç CriticAgent: Evaluating results...")
                time.sleep(0.5)
                
                print(f"  [{iteration}/6] üí° ReflectorAgent: Analyzing performance...")
                time.sleep(0.5)
                
                print(f"  [{iteration}/6] ‚úì  Checking target metrics...")
                
                iter_time = time.time() - iter_start
                
                # Mock improving metrics
                mock_metrics = {
                    "sharpe": round(0.5 + (iteration * 0.4), 2),
                    "maxdd": round(-0.30 + (iteration * 0.03), 2),
                    "ann_ret": round(0.10 + (iteration * 0.06), 2),
                    "turnover_monthly": round(80 - (iteration * 5), 1),
                    "avg_ic": round(0.04 + (iteration * 0.02), 3)
                }
                
                # Determine status
                meets_sharpe = mock_metrics['sharpe'] >= 1.8
                meets_dd = mock_metrics['maxdd'] >= -0.25
                status = "PASS" if (meets_sharpe and meets_dd) else "FAIL"
                
                iteration_result = {
                    "iteration": iteration,
                    "alpha_id": alpha_id,
                    "runtime_seconds": round(iter_time, 2),
                    "metrics": mock_metrics,
                    "status": status,
                    "meets_sharpe": meets_sharpe,
                    "meets_drawdown": meets_dd
                }
                
                results["iterations"].append(iteration_result)
                
                # Print results
                print(f"\n  üìà Results:")
                print(f"     Sharpe Ratio: {mock_metrics['sharpe']:.2f} {'‚úÖ' if meets_sharpe else '‚ùå'} (target: ‚â•1.8)")
                print(f"     Max Drawdown: {mock_metrics['maxdd']:.1%} {'‚úÖ' if meets_dd else '‚ùå'} (target: ‚â•-25%)")
                print(f"     Annual Return: {mock_metrics['ann_ret']:.1%}")
                print(f"     Turnover: {mock_metrics['turnover_monthly']:.1f}%")
                print(f"     Avg IC: {mock_metrics['avg_ic']:.3f}")
                print(f"     Runtime: {iter_time:.2f}s")
                print(f"     Status: {status}\n")
                
                # Check if target met
                if status == "PASS":
                    print(f"  üéâ SUCCESS! Target metrics achieved at iteration {iteration}\n")
                    results["status"] = "SUCCESS"
                    results["success_iteration"] = iteration
                    break
                    
            except Exception as e:
                print(f"  ‚ùå Iteration {iteration} failed: {e}\n")
                results["iterations"].append({
                    "iteration": iteration,
                    "alpha_id": alpha_id,
                    "status": "ERROR",
                    "error": str(e),
                    "runtime_seconds": time.time() - iter_start
                })
                
        if results["status"] == "RUNNING":
            results["status"] = "COMPLETED_NO_SUCCESS"
            print(f"  ‚ö†Ô∏è  Completed {max_iterations} iterations without reaching target\n")
            
    except Exception as e:
        print(f"\n‚ùå Discovery loop failed: {e}\n")
        results["status"] = "FAILED"
        results["error"] = str(e)
    
    # Calculate total runtime
    total_runtime = time.time() - test_start
    results["total_runtime_seconds"] = round(total_runtime, 2)
    results["total_runtime_minutes"] = round(total_runtime / 60, 2)
    
    # Save results
    results_file = output_dir / "test_results.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    
    # Print summary
    print(f"{'='*70}")
    print(f"  TEST COMPLETE: {model_name}")
    print(f"{'='*70}")
    print(f"Status: {results['status']}")
    print(f"Iterations Run: {len(results['iterations'])}")
    print(f"Total Runtime: {results['total_runtime_minutes']:.2f} minutes")
    print(f"Results Saved: {results_file}")
    print(f"{'='*70}\n")
    
    return results


def compare_models_simple(
    models: List[str],
    max_iterations: int = 3
) -> Dict[str, Any]:
    """
    Compare multiple models (simplified version).
    
    Args:
        models: List of model names to test
        max_iterations: Max iterations per model
        
    Returns:
        Comparison results
    """
    print(f"\n{'#'*70}")
    print(f"  MODEL COMPARISON TEST")
    print(f"{'#'*70}")
    print(f"Models to Test: {len(models)}")
    print(f"  - {chr(10).join('  - ' + m for m in models)}")
    print(f"Max Iterations per Model: {max_iterations}")
    print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'#'*70}\n")
    
    comparison_start = time.time()
    
    comparison = {
        "timestamp": datetime.now().isoformat(),
        "models_tested": models,
        "max_iterations": max_iterations,
        "results": {}
    }
    
    for i, model in enumerate(models, 1):
        print(f"\n{'‚ñà'*70}")
        print(f"  MODEL {i}/{len(models)}: {model}")
        print(f"{'‚ñà'*70}\n")
        
        try:
            result = test_discovery_loop_simple(
                model_name=model,
                max_iterations=max_iterations
            )
            comparison["results"][model] = result
        except Exception as e:
            print(f"‚ùå Model {model} test failed: {e}\n")
            comparison["results"][model] = {
                "status": "FAILED",
                "error": str(e)
            }
    
    # Calculate total time
    total_time = time.time() - comparison_start
    comparison["total_runtime_minutes"] = round(total_time / 60, 2)
    
    # Generate comparison summary
    print(f"\n{'#'*70}")
    print(f"  COMPARISON SUMMARY")
    print(f"{'#'*70}\n")
    
    summary_data = []
    for model, result in comparison["results"].items():
        if result.get("status") != "FAILED":
            summary_data.append({
                "model": model,
                "status": result.get("status", "UNKNOWN"),
                "iterations": len(result.get("iterations", [])),
                "runtime_min": result.get("total_runtime_minutes", 0),
                "success_iter": result.get("success_iteration", "-")
            })
    
    # Print comparison table
    print(f"{'Model':<25} {'Status':<20} {'Iters':<8} {'Time(min)':<12} {'Success@':<10}")
    print(f"{'-'*80}")
    for row in summary_data:
        print(f"{row['model']:<25} {row['status']:<20} {row['iterations']:<8} "
              f"{row['runtime_min']:<12.2f} {str(row['success_iter']):<10}")
    
    print(f"\n{'‚îÄ'*80}")
    print(f"Total Test Time: {comparison['total_runtime_minutes']:.2f} minutes")
    print(f"{'‚îÄ'*80}\n")
    
    # Save comparison
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    comparison_file = Path("test_results/integration_tests") / f"model_comparison_{timestamp}.json"
    comparison_file.parent.mkdir(parents=True, exist_ok=True)
    
    with open(comparison_file, 'w') as f:
        json.dump(comparison, f, indent=2)
    
    print(f"üìä Full comparison saved to: {comparison_file}\n")
    
    return comparison


if __name__ == "__main__":
    # Models to test (from fastest to slowest)
    models_to_test = [
        "deepseek-r1:1.5b",      # Fastest, lightweight
        "llama3.2:3b",           # Fast, good quality
        "qwen2.5:7b",            # Medium speed, high quality
        "deepseek-r1"            # Default (slower but better)
    ]
    
    print("\n" + "="*70)
    print("  QUANTALPHA FULL INTEGRATION TEST")
    print("="*70)
    print(f"Test Suite: Model Comparison + Performance Benchmarking")
    print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Models: {len(models_to_test)}")
    print(f"Iterations per Model: 3")
    print("="*70 + "\n")
    
    # Run comparison
    try:
        comparison = compare_models_simple(
            models=models_to_test,
            max_iterations=3
        )
        
        print("\n" + "="*70)
        print("  ‚úÖ FULL INTEGRATION TEST COMPLETE")
        print("="*70)
        print(f"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Total Runtime: {comparison['total_runtime_minutes']:.2f} minutes")
        print(f"Results Directory: test_results/integration_tests/")
        print("="*70 + "\n")
        
    except Exception as e:
        print(f"\n‚ùå Test suite failed: {e}\n")
        raise
